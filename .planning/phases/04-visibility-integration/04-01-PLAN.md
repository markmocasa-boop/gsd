---
phase: 04-visibility-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - lambdas/slack_notifier/handler.py
  - lambdas/slack_notifier/requirements.txt
  - lambdas/email_notifier/handler.py
  - lambdas/email_notifier/requirements.txt
  - infra/lib/notification-stack.ts
  - infra/bin/data-foundations.ts
  - supabase/migrations/005_quality_trends.sql
autonomous: true
user_setup:
  - service: slack
    why: "Incoming webhook for alert notifications"
    env_vars:
      - name: SLACK_WEBHOOK_URL
        source: "Slack App > Incoming Webhooks > Add New Webhook"
    dashboard_config:
      - task: "Create Slack App with Incoming Webhooks"
        location: "https://api.slack.com/apps > Create New App"
  - service: aws-ses
    why: "Email notifications for alerts"
    env_vars:
      - name: SES_SENDER_EMAIL
        source: "Verified email identity in SES console"
    dashboard_config:
      - task: "Verify sender email in SES (sandbox mode allows only verified recipients)"
        location: "AWS Console > SES > Verified Identities"

must_haves:
  truths:
    - "Slack receives notification when AlertCreated event fires"
    - "Email notification sent when AlertCreated event fires"
    - "Quality trends can be queried for last 30 days"
    - "Composite quality score calculated from dimension weights"
  artifacts:
    - path: "lambdas/slack_notifier/handler.py"
      provides: "Slack Block Kit message handler"
      min_lines: 50
    - path: "lambdas/email_notifier/handler.py"
      provides: "SES email sender"
      min_lines: 50
    - path: "infra/lib/notification-stack.ts"
      provides: "CDK stack for notification Lambdas"
      exports: ["NotificationStack"]
    - path: "supabase/migrations/005_quality_trends.sql"
      provides: "Trend aggregation functions"
      contains: "get_quality_trends"
  key_links:
    - from: "infra/lib/notification-stack.ts"
      to: "EventBridge AlertCreated"
      via: "events.Rule with source=data-quality.alerts"
      pattern: "source.*data-quality\\.alerts"
    - from: "lambdas/slack_notifier/handler.py"
      to: "Slack webhook"
      via: "urllib.request.urlopen"
      pattern: "urlopen.*webhook"
    - from: "lambdas/email_notifier/handler.py"
      to: "AWS SES"
      via: "boto3 SES client"
      pattern: "ses.*send"
---

<objective>
Create notification infrastructure for alert delivery and database functions for quality trend aggregation.

Purpose: Enable users to receive Slack/email notifications when quality issues are detected (VIS-01), and provide database functions for calculating composite quality scores and historical trends (VIS-03, VIS-04).

Output: Two notification Lambda functions subscribing to AlertCreated events, CDK stack for deployment, and PostgreSQL functions for trend aggregation.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-visibility-integration/04-RESEARCH.md

# Existing alert infrastructure
@lambdas/alert_handler/handler.py
@infra/lib/alerting-stack.ts
@supabase/migrations/003_validation_results.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create notification Lambda functions</name>
  <files>
    lambdas/slack_notifier/handler.py
    lambdas/slack_notifier/requirements.txt
    lambdas/email_notifier/handler.py
    lambdas/email_notifier/requirements.txt
  </files>
  <action>
Create two Lambda functions that subscribe to AlertCreated events from EventBridge:

**slack_notifier/handler.py:**
- Use aws-lambda-powertools for Logger and Tracer (match alert_handler pattern)
- Extract event detail: alert_id, dataset_id, severity, title, message, timestamp
- Build Slack Block Kit message with:
  - Header block with severity emoji (critical=rotating_light, warning=warning, info=information_source)
  - Section with severity and alert_id fields
  - Section with message text
  - Context block with timestamp
- POST to SLACK_WEBHOOK_URL environment variable using urllib.request (no external deps)
- Return {"status": "sent", "channel": "slack"}
- Handle errors gracefully, log but don't fail on webhook errors

**email_notifier/handler.py:**
- Use aws-lambda-powertools for Logger and Tracer
- Use boto3 SES client (ses_client = boto3.client('ses'))
- Build HTML email with:
  - Subject: f"[{severity.upper()}] {title}"
  - HTML body with alert details table (Alert ID, Severity, Message, Time)
  - Plain text fallback
- Send via ses_client.send_email() to ALERT_EMAIL_RECIPIENTS env var (comma-separated)
- Use SES_SENDER_EMAIL env var for From address
- Return {"status": "sent", "channel": "email", "recipient_count": N}

**requirements.txt for both:**
```
aws-lambda-powertools>=2.0.0
```
(boto3 is provided by Lambda runtime)

Do NOT use external HTTP libraries - use stdlib urllib.request for Slack.
  </action>
  <verify>
    `python -m py_compile lambdas/slack_notifier/handler.py && python -m py_compile lambdas/email_notifier/handler.py` succeeds
  </verify>
  <done>
    Both Lambda handlers compile without errors, use Powertools pattern, handle AlertCreated event structure
  </done>
</task>

<task type="auto">
  <name>Task 2: Create notification CDK stack</name>
  <files>
    infra/lib/notification-stack.ts
    infra/bin/data-foundations.ts
  </files>
  <action>
Create CDK stack that deploys notification Lambdas and wires EventBridge rules:

**notification-stack.ts:**
- Import cdk, lambda, events, targets, logs, iam, secretsmanager
- Export class NotificationStack extends cdk.Stack
- Define interface NotificationStackProps with optional alertingStackRef for cross-stack references

**Slack Notifier Lambda:**
- Function name: 'dq-slack-notifier'
- Runtime: Python 3.11
- Code from asset '../lambdas/slack_notifier'
- Timeout: 30 seconds, Memory: 256MB
- Environment: POWERTOOLS_SERVICE_NAME, POWERTOOLS_LOG_LEVEL
- Create SSM Parameter or Secrets Manager reference for SLACK_WEBHOOK_URL
- Log group: /aws/lambda/dq-slack-notifier (1 month retention)

**Email Notifier Lambda:**
- Function name: 'dq-email-notifier'
- Runtime: Python 3.11
- Code from asset '../lambdas/email_notifier'
- Timeout: 30 seconds, Memory: 256MB
- Environment: POWERTOOLS_SERVICE_NAME, ALERT_EMAIL_RECIPIENTS, SES_SENDER_EMAIL
- Grant SES send permissions: ses:SendEmail, ses:SendRawEmail
- Log group: /aws/lambda/dq-email-notifier (1 month retention)

**EventBridge Rules:**
- Rule 'dq-alert-slack-notify':
  - Pattern: source=['data-quality.alerts'], detailType=['AlertCreated']
  - Target: slackNotifier Lambda with 2 retry attempts
- Rule 'dq-alert-email-notify':
  - Pattern: source=['data-quality.alerts'], detailType=['AlertCreated']
  - Target: emailNotifier Lambda with 2 retry attempts

**Outputs:**
- SlackNotifierArn, EmailNotifierArn

**bin/data-foundations.ts:**
- Add `new NotificationStack(app, 'NotificationStack', { env })` after AlertingStack
  </action>
  <verify>
    `cd infra && npm run build` succeeds without TypeScript errors
  </verify>
  <done>
    NotificationStack deploys two Lambdas with EventBridge rules subscribing to AlertCreated events
  </done>
</task>

<task type="auto">
  <name>Task 3: Create quality trends migration</name>
  <files>
    supabase/migrations/005_quality_trends.sql
  </files>
  <action>
Create PostgreSQL migration with functions for quality trend aggregation:

**Function 1: get_quality_trends(p_dataset_id UUID, p_days INTEGER DEFAULT 30)**
Returns daily aggregated scores for trend charts:
```sql
CREATE OR REPLACE FUNCTION get_quality_trends(
  p_dataset_id UUID,
  p_days INTEGER DEFAULT 30
) RETURNS TABLE (
  day DATE,
  dimension VARCHAR(50),
  avg_score DECIMAL(5,4),
  rolling_7day_avg DECIMAL(5,4)
) AS $$
BEGIN
  RETURN QUERY
  WITH daily_scores AS (
    SELECT
      DATE_TRUNC('day', measured_at)::DATE AS day,
      qs.dimension,
      AVG(score) AS avg_score
    FROM quality_scores qs
    WHERE qs.dataset_id = p_dataset_id
      AND qs.measured_at >= NOW() - (p_days || ' days')::INTERVAL
    GROUP BY DATE_TRUNC('day', measured_at)::DATE, qs.dimension
  )
  SELECT
    ds.day,
    ds.dimension,
    ds.avg_score,
    AVG(ds.avg_score) OVER (
      PARTITION BY ds.dimension
      ORDER BY ds.day
      ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
    ) AS rolling_7day_avg
  FROM daily_scores ds
  ORDER BY ds.day, ds.dimension;
END;
$$ LANGUAGE plpgsql STABLE;
```

**Function 2: get_composite_quality_score(p_dataset_id UUID)**
Returns weighted composite score using dimension weights:
- completeness: 0.25, validity: 0.25, uniqueness: 0.20, consistency: 0.15, freshness: 0.15
```sql
CREATE OR REPLACE FUNCTION get_composite_quality_score(p_dataset_id UUID)
RETURNS DECIMAL(5,4) AS $$
DECLARE
  v_score DECIMAL(5,4);
BEGIN
  WITH latest_scores AS (
    SELECT DISTINCT ON (dimension)
      dimension,
      score
    FROM quality_scores
    WHERE dataset_id = p_dataset_id
    ORDER BY dimension, measured_at DESC
  ),
  weighted AS (
    SELECT
      SUM(
        CASE dimension
          WHEN 'completeness' THEN score * 0.25
          WHEN 'validity' THEN score * 0.25
          WHEN 'uniqueness' THEN score * 0.20
          WHEN 'consistency' THEN score * 0.15
          WHEN 'freshness' THEN score * 0.15
          ELSE 0
        END
      ) AS weighted_sum,
      SUM(
        CASE dimension
          WHEN 'completeness' THEN 0.25
          WHEN 'validity' THEN 0.25
          WHEN 'uniqueness' THEN 0.20
          WHEN 'consistency' THEN 0.15
          WHEN 'freshness' THEN 0.15
          ELSE 0
        END
      ) AS total_weight
    FROM latest_scores
  )
  SELECT
    CASE WHEN total_weight > 0
      THEN weighted_sum / total_weight
      ELSE NULL
    END INTO v_score
  FROM weighted;

  RETURN v_score;
END;
$$ LANGUAGE plpgsql STABLE;
```

**Function 3: get_dashboard_stats()**
Returns aggregated stats for dashboard overview:
```sql
CREATE OR REPLACE FUNCTION get_dashboard_stats()
RETURNS TABLE (
  total_datasets BIGINT,
  avg_composite_score DECIMAL(5,4),
  datasets_above_threshold BIGINT,
  datasets_below_threshold BIGINT,
  open_alerts BIGINT,
  critical_alerts BIGINT
) AS $$
BEGIN
  RETURN QUERY
  WITH dataset_scores AS (
    SELECT
      d.id,
      get_composite_quality_score(d.id) AS composite_score
    FROM datasets d
  )
  SELECT
    (SELECT COUNT(*) FROM datasets)::BIGINT AS total_datasets,
    (SELECT AVG(composite_score) FROM dataset_scores WHERE composite_score IS NOT NULL) AS avg_composite_score,
    (SELECT COUNT(*) FROM dataset_scores WHERE composite_score >= 0.8)::BIGINT AS datasets_above_threshold,
    (SELECT COUNT(*) FROM dataset_scores WHERE composite_score < 0.8 AND composite_score IS NOT NULL)::BIGINT AS datasets_below_threshold,
    (SELECT COUNT(*) FROM alerts WHERE status = 'open')::BIGINT AS open_alerts,
    (SELECT COUNT(*) FROM alerts WHERE status = 'open' AND severity = 'critical')::BIGINT AS critical_alerts;
END;
$$ LANGUAGE plpgsql STABLE;
```

Add COMMENT for each function. Grant EXECUTE to authenticated role.
  </action>
  <verify>
    Migration file contains valid SQL with CREATE FUNCTION statements for get_quality_trends, get_composite_quality_score, get_dashboard_stats
  </verify>
  <done>
    Three PostgreSQL functions created for trend aggregation, composite scoring, and dashboard stats
  </done>
</task>

</tasks>

<verification>
1. Lambda handlers compile: `python -m py_compile lambdas/slack_notifier/handler.py && python -m py_compile lambdas/email_notifier/handler.py`
2. CDK compiles: `cd infra && npm run build`
3. Migration file exists and contains all three functions
4. EventBridge rules target AlertCreated events from data-quality.alerts source
</verification>

<success_criteria>
- VIS-01 infrastructure ready: Slack and email notification Lambdas created
- VIS-03/VIS-04 database support: Trend aggregation and composite scoring functions created
- All files compile without errors
- EventBridge fan-out pattern implemented (alert_handler -> AlertCreated -> slack/email)
</success_criteria>

<output>
After completion, create `.planning/phases/04-visibility-integration/04-01-SUMMARY.md`
</output>
