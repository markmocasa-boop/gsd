---
phase: 02-data-quality-ai-recommendations
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - lambdas/validator/handler.py
  - lambdas/validator/requirements.txt
  - lambdas/alert_handler/handler.py
  - lambdas/alert_handler/requirements.txt
  - lambdas/freshness_monitor/handler.py
  - lambdas/freshness_monitor/requirements.txt
  - supabase/migrations/003_validation_results.sql
  - infra/lib/validator-stack.ts
  - infra/lib/alerting-stack.ts
  - backend/api/validations/route.ts
  - backend/api/validations/[id]/route.ts
  - backend/api/rules/route.ts
  - backend/api/rules/[id]/route.ts
  - backend/api/rules/generate/route.ts
  - backend/api/alerts/route.ts
  - backend/api/alerts/[id]/route.ts
autonomous: true

must_haves:
  truths:
    - "Quality checks (null, unique, range, referential integrity) can execute against data sources"
    - "Validation results are stored with pass/fail status per rule"
    - "Alerts trigger when data freshness exceeds SLA or volume anomalies detected"
    - "Quality validations integrate with Step Functions for pipeline orchestration"
    - "DQ Recommender agent can be invoked via API to generate rules from natural language"
  artifacts:
    - path: "lambdas/validator/handler.py"
      provides: "Glue Data Quality execution handler"
      exports: ["handler"]
    - path: "lambdas/alert_handler/handler.py"
      provides: "Alert processing and notification"
      exports: ["handler"]
    - path: "lambdas/freshness_monitor/handler.py"
      provides: "Scheduled freshness and volume checks"
      exports: ["handler"]
    - path: "supabase/migrations/003_validation_results.sql"
      provides: "Schema for validation runs and alerts"
      contains: "CREATE TABLE validation_runs"
    - path: "infra/lib/validator-stack.ts"
      provides: "Step Functions validation workflow"
      exports: ["ValidatorStack"]
    - path: "backend/api/rules/generate/route.ts"
      provides: "API endpoint to invoke DQ Recommender agent"
      exports: ["POST"]
  key_links:
    - from: "lambdas/validator/handler.py"
      to: "AWS Glue Data Quality"
      via: "boto3 glue client"
      pattern: "glue\\.start_data_quality_ruleset_evaluation_run"
    - from: "infra/lib/validator-stack.ts"
      to: "lambdas/approval_handler"
      via: "Step Functions waitForTaskToken"
      pattern: "waitForTaskToken|TaskToken"
    - from: "lambdas/alert_handler/handler.py"
      to: "EventBridge"
      via: "Event source pattern"
      pattern: "events\\.put_events|EventBridge"
    - from: "backend/api/rules/generate/route.ts"
      to: "agents/dq_recommender/agent.py"
      via: "Agent invocation"
      pattern: "dq_recommender_agent|generate_dqdl_rule"
---

<objective>
Build the Data Validator system that executes quality checks with Step Functions orchestration and freshness monitoring.

Purpose: Enable automated data quality validation with pipeline integration, implementing the core DQ checks (null, unique, range, referential integrity) and proactive monitoring for freshness SLA violations and volume anomalies.

Output: Validation workflow with Glue DQ integration, freshness monitoring, alert system, and API endpoints including DQ Recommender invocation.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-data-quality-ai-recommendations/02-RESEARCH.md
@.planning/phases/02-data-quality-ai-recommendations/02-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Validation Lambdas and Database Schema</name>
  <files>
    lambdas/validator/handler.py
    lambdas/validator/requirements.txt
    lambdas/alert_handler/handler.py
    lambdas/alert_handler/requirements.txt
    lambdas/freshness_monitor/handler.py
    lambdas/freshness_monitor/requirements.txt
    supabase/migrations/003_validation_results.sql
  </files>
  <action>
    Create validation execution and monitoring Lambdas with database schema:

    **supabase/migrations/003_validation_results.sql:**
    Create tables following research schema:

    1. `validation_runs` - Job execution tracking:
       - id UUID PK, dataset_id FK references datasets(id) ON DELETE CASCADE
       - glue_run_id VARCHAR(255), step_function_execution_arn VARCHAR(500)
       - status VARCHAR(50) NOT NULL (pending, running, completed, failed)
       - started_at, completed_at TIMESTAMPTZ
       - triggered_by VARCHAR(50), trigger_details JSONB
       - overall_score DECIMAL(5,4), rules_evaluated INTEGER, rules_passed INTEGER, rules_failed INTEGER
       - s3_results_uri VARCHAR(500)
       - created_at TIMESTAMPTZ

    2. `rule_results` - Per-rule execution results:
       - id UUID PK, run_id FK references validation_runs(id) ON DELETE CASCADE
       - rule_id FK references dq_rules(id) ON DELETE SET NULL
       - result VARCHAR(20) NOT NULL (pass, fail, error, skip)
       - evaluation_message TEXT
       - evaluated_count, passed_count, failed_count BIGINT
       - sample_failures JSONB
       - created_at TIMESTAMPTZ

    3. `quality_scores` - Time series metrics:
       - id UUID PK, dataset_id FK references datasets(id)
       - dimension VARCHAR(50) NOT NULL (completeness, validity, uniqueness, consistency, freshness)
       - score DECIMAL(5,4) NOT NULL
       - run_id FK references validation_runs(id)
       - measured_at TIMESTAMPTZ

    4. `alerts` - Alert records:
       - id UUID PK, dataset_id FK, rule_id FK, run_id FK (all nullable)
       - alert_type VARCHAR(50) NOT NULL (rule_failure, freshness_sla, volume_anomaly)
       - severity VARCHAR(20) NOT NULL, title VARCHAR(255) NOT NULL
       - message TEXT, details JSONB
       - status VARCHAR(20) DEFAULT 'open' (open, acknowledged, resolved, snoozed)
       - acknowledged_by UUID, acknowledged_at TIMESTAMPTZ
       - resolved_by UUID, resolved_at TIMESTAMPTZ, resolution_notes TEXT
       - notification_sent BOOLEAN DEFAULT FALSE, notification_channels JSONB
       - created_at TIMESTAMPTZ

    5. `freshness_slas` - SLA configuration:
       - id UUID PK, dataset_id FK references datasets(id) UNIQUE
       - max_age_hours INTEGER NOT NULL
       - check_schedule VARCHAR(100) (cron expression)
       - severity VARCHAR(20) DEFAULT 'warning'
       - enabled BOOLEAN DEFAULT TRUE
       - last_check_at, last_violation_at TIMESTAMPTZ
       - created_at TIMESTAMPTZ

    **Indexes:** validation_runs(dataset_id, status), rule_results(run_id), quality_scores(dataset_id, measured_at), alerts(status, severity)
    **RLS:** Enable on all tables with appropriate policies

    **lambdas/validator/handler.py:**
    Process validation results from Glue Data Quality (from research patterns):
    - Input: run_id (Glue DQ run ID), dataset_id, ruleset_names
    - Call glue.get_data_quality_ruleset_evaluation_run to get results
    - Call glue.batch_get_data_quality_result for detailed rule results
    - Calculate overall_score (passed / total)
    - Store in validation_runs and rule_results tables
    - Calculate dimension scores (group rules by type: completeness, validity, etc.)
    - Store in quality_scores table
    - Update dq_rules.last_triggered_at and trigger_count for failed rules
    - Return processed results summary

    **lambdas/alert_handler/handler.py:**
    Process alerts from EventBridge events:
    - Input: EventBridge event with alert details (type, severity, dataset, details)
    - Create alert record in alerts table
    - Check notification preferences (stored in dataset or user config)
    - Emit to EventBridge for downstream handlers (Slack/email in Phase 4)
    - Track notification_sent status
    - Return alert_id

    **lambdas/freshness_monitor/handler.py:**
    Scheduled Lambda for freshness and volume monitoring (from research):
    - Query freshness_slas table for enabled SLAs
    - For each dataset with SLA:
      - Get table metadata from Glue catalog (UpdateTime)
      - Calculate age in hours
      - If age > max_age_hours: emit FreshnessSLAViolation event
    - Volume anomaly check:
      - Compare current row count to last 7 runs
      - If < 50% or > 200% of average: emit VolumeAnomaly event
    - Update last_check_at in freshness_slas

    **requirements.txt for each Lambda:**
    ```
    boto3>=1.35.0
    aws-lambda-powertools>=3.0.0
    supabase>=2.0.0
    ```
  </action>
  <verify>
    # Check SQL syntax
    cat supabase/migrations/003_validation_results.sql | grep -E "CREATE TABLE" | wc -l
    # Should show 5 tables

    # Check Lambda handlers exist
    ls lambdas/validator/handler.py lambdas/alert_handler/handler.py lambdas/freshness_monitor/handler.py
  </verify>
  <done>
    Migration creates 5 tables (validation_runs, rule_results, quality_scores, alerts, freshness_slas). Validator Lambda processes Glue DQ results. Alert handler creates records and emits events. Freshness monitor checks SLAs and volume.
  </done>
</task>

<task type="auto">
  <name>Task 2: CDK Infrastructure and Step Functions Workflow</name>
  <files>
    infra/lib/validator-stack.ts
    infra/lib/alerting-stack.ts
    infra/bin/app.ts
  </files>
  <action>
    Create CDK stacks for validation workflow and alerting:

    **infra/lib/validator-stack.ts:**
    Create ValidatorStack with Step Functions workflow from research:

    1. **Lambda functions:**
       - validator_processor: Process Glue DQ results (lambdas/validator)
       - results_storer: Store final results (or inline in validator)

    2. **Step Functions State Machine** (ValidationWorkflow):
       States following research pattern:
       ```
       CheckRuleStatus -> (pending) -> RequestApproval -> ExecuteValidation
                      -> (approved) -> ExecuteValidation

       RequestApproval: waitForTaskToken (SQS message to approval queue)
         - TimeoutSeconds: 86400 (24 hour approval window)
         - Catch timeout -> ApprovalTimedOut

       ExecuteValidation: glue:startDataQualityRulesetEvaluationRun.sync
         - Parameters: DataSource (GlueTable), RulesetNames
         - Retry on ConcurrentRunsExceededException (3 attempts, backoff)
         - Catch errors -> HandleValidationError

       ProcessResults: Lambda invoke validator_processor
         - Pass Glue results for processing

       CheckQualityScore: Choice state
         - If score < 0.8 -> TriggerAlert
         - Else -> StoreResults

       TriggerAlert: events:putEvents (QualityCheckFailed)

       StoreResults: Lambda invoke results_storer (or combined with ProcessResults)

       HandleValidationError: SNS publish error notification
       ApprovalTimedOut: Lambda notify timeout, then Fail
       ```

    3. **SQS Queue** for approval requests (approval-queue)
    4. **IAM roles** with Glue DQ, Lambda invoke, EventBridge, SQS permissions
    5. **Export** state machine ARN for API integration

    **infra/lib/alerting-stack.ts:**
    Create AlertingStack:

    1. **Lambda functions:**
       - alert_handler: Process alert events
       - freshness_monitor: Scheduled freshness checks

    2. **EventBridge rules:**
       - QualityCheckFailed -> alert_handler
       - FreshnessSLAViolation -> alert_handler
       - VolumeAnomaly -> alert_handler
       - Scheduled rule for freshness_monitor (every 15 minutes)

    3. **SNS Topic** for error notifications (dq-alerts)
    4. **EventBridge event bus** for DQ events (or use default)

    **Update infra/bin/app.ts:**
    - Add ValidatorStack (depends on DQRecommenderStack for approval Lambda)
    - Add AlertingStack (depends on ValidatorStack for EventBridge sources)
    - Wire dependencies correctly
  </action>
  <verify>
    cd infra && npx cdk synth --quiet
    # Should synthesize without errors

    # Check state machine definition exists
    grep -r "StateMachine" infra/lib/validator-stack.ts | head -3
  </verify>
  <done>
    CDK synthesizes with ValidatorStack and AlertingStack. Step Functions workflow implements approval -> validation -> results flow. EventBridge rules route quality events to alert handler. Freshness monitor runs on schedule.
  </done>
</task>

<task type="auto">
  <name>Task 3: Backend API Endpoints with DQ Recommender Integration</name>
  <files>
    backend/api/validations/route.ts
    backend/api/validations/[id]/route.ts
    backend/api/rules/route.ts
    backend/api/rules/[id]/route.ts
    backend/api/rules/generate/route.ts
    backend/api/alerts/route.ts
    backend/api/alerts/[id]/route.ts
  </files>
  <action>
    Create API endpoints for validation management and DQ Recommender invocation:

    **backend/api/rules/generate/route.ts:**
    POST endpoint to invoke DQ Recommender agent for rule generation:
    - Receives JSON body: { description: string, column_name: string, dataset_id: string, profile_summary?: object }
    - Validates input with Zod schema
    - Loads profile summary from database if not provided (query profiles table for column stats)
    - Invokes DQ Recommender agent:
      - Import dq_recommender_agent from agents/dq_recommender
      - Call agent with natural language description + column context
      - OR call generate_dqdl_rule tool directly for simpler integration
    - Returns JSON response:
      ```typescript
      {
        data: {
          rule: string,           // Generated DQDL expression
          reasoning: string,      // AI explanation of why this rule
          false_positive_scenarios: string[], // Potential false positive cases
          severity: 'critical' | 'warning' | 'info',
          suggested_rule_type: string
        }
      }
      ```
    - Error handling: 400 for invalid input, 500 for agent errors
    - Consider timeout: agent may take 5-15 seconds, set appropriate timeout

    **backend/api/validations/route.ts:**
    - GET: List validation_runs with filters (dataset_id, status, date range)
      - Include overall_score, rules_passed, rules_failed
      - Pagination support
    - POST: Trigger new validation run
      1. Validate dataset_id and rule_ids (or "all active rules")
      2. Create validation_runs record with status='pending'
      3. Get active dq_rules for dataset
      4. Build ruleset from DQDL expressions
      5. Start ValidationWorkflow Step Functions execution
      6. Update validation_runs with execution ARN
      7. Return run_id for polling

    **backend/api/validations/[id]/route.ts:**
    - GET: Get validation_run by ID with full results
      - Include rule_results array with rule details
      - Include quality_scores by dimension
      - If running, return status only (for polling)
    - DELETE: Cancel running validation (if supported)

    **backend/api/rules/route.ts:**
    - GET: List dq_rules with filters (dataset_id, status, rule_type, severity)
    - POST: Create new rule
      - If generated_by='ai_recommender', status='pending'
      - If generated_by='user', can optionally skip approval
      - Validate DQDL expression format (basic syntax check)

    **backend/api/rules/[id]/route.ts:**
    - GET: Get rule by ID
    - PUT: Update rule (only if status='pending' or 'active')
    - DELETE: Delete rule (soft delete: set status='deprecated')
    - POST /approve: Approve pending rule (wrapper for approval Lambda)
    - POST /reject: Reject pending rule

    **backend/api/alerts/route.ts:**
    - GET: List alerts with filters (status, severity, dataset_id, date range)
      - Default: open alerts sorted by severity, created_at
    - Pagination support

    **backend/api/alerts/[id]/route.ts:**
    - GET: Get alert by ID with full details
    - PUT: Update alert status (acknowledge, resolve, snooze)
      - acknowledged_by/resolved_by set from auth context
      - resolution_notes required for resolve

    Use Zod for request validation. Return consistent { data, error } structure.
    Add appropriate TypeScript types for all entities.
  </action>
  <verify>
    cd backend && npx tsc --noEmit
    # TypeScript compiles without errors

    # Check all route files exist including generate
    ls backend/api/validations/route.ts backend/api/rules/route.ts backend/api/rules/generate/route.ts backend/api/alerts/route.ts
  </verify>
  <done>
    API endpoints compile successfully. /api/rules/generate invokes DQ Recommender agent and returns rule with reasoning. Validations endpoint triggers Step Functions workflow. Rules endpoint supports CRUD with approval flow. Alerts endpoint allows status management.
  </done>
</task>

</tasks>

<verification>
1. SQL migration has valid syntax with 5 tables
2. All Lambda handlers have correct boto3/Supabase integration
3. CDK synthesizes with Step Functions and EventBridge resources
4. `cd backend && npx tsc --noEmit` succeeds
5. Step Functions workflow includes approval, validation, alert states
6. Freshness monitor checks run on EventBridge schedule
7. /api/rules/generate endpoint exists and invokes DQ Recommender agent
</verification>

<success_criteria>
- Validation workflow executes Glue DQ rules with retry handling
- Human approval gate uses waitForTaskToken pattern
- Results processor calculates overall score and dimension scores
- Alerts created for score < 0.8, freshness violations, volume anomalies
- Freshness monitor runs every 15 minutes checking SLAs
- API endpoints: trigger validation, get results, manage rules, manage alerts
- /api/rules/generate invokes DQ Recommender and returns DQDL rule with reasoning (AI-01 satisfied)
- All quality checks (null, unique, range, referential) supported via DQDL
- INT-03 satisfied: Step Functions pipeline integration
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-quality-ai-recommendations/02-02-SUMMARY.md`
</output>
