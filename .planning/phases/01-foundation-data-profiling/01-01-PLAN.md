---
phase: 01-foundation-data-profiling
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - infra/lib/profiler-stack.ts
  - infra/lib/data-stack.ts
  - infra/bin/app.ts
  - infra/package.json
  - infra/tsconfig.json
  - infra/cdk.json
  - supabase/migrations/001_profile_schema.sql
  - agents/profiler/tools/connectors.py
  - agents/profiler/schemas.py
  - agents/profiler/requirements.txt
autonomous: true

must_haves:
  truths:
    - "CDK can be deployed to create Fargate cluster and Step Functions state machine"
    - "Supabase schema supports storing data source connections and profile results"
    - "Python connector classes can retrieve data from Iceberg, Redshift, and Athena"
  artifacts:
    - path: "infra/lib/profiler-stack.ts"
      provides: "Fargate cluster, Step Functions state machine, S3 bucket"
      exports: ["ProfilerStack"]
    - path: "supabase/migrations/001_profile_schema.sql"
      provides: "Database schema for profiles"
      contains: "CREATE TABLE data_sources"
    - path: "agents/profiler/tools/connectors.py"
      provides: "Data connector implementations"
      exports: ["DataConnector", "IcebergConnector", "RedshiftConnector", "AthenaConnector"]
  key_links:
    - from: "infra/lib/profiler-stack.ts"
      to: "AWS ECS/Step Functions"
      via: "CDK constructs"
      pattern: "new ecs\\.Cluster|new sfn\\.StateMachine"
    - from: "agents/profiler/tools/connectors.py"
      to: "PyIceberg/boto3"
      via: "SDK calls"
      pattern: "load_catalog|boto3\\.client"
---

<objective>
Set up foundational infrastructure for the Data Profiler system.

Purpose: Establish the AWS infrastructure (Fargate, Step Functions, S3), database schema for storing profiles, and data connector classes that all subsequent plans depend on.

Output: Deployable CDK stack, runnable Supabase migration, and importable Python connector module.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation-data-profiling/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: CDK Infrastructure Setup</name>
  <files>
    infra/lib/profiler-stack.ts
    infra/lib/data-stack.ts
    infra/bin/app.ts
    infra/package.json
    infra/tsconfig.json
    infra/cdk.json
  </files>
  <action>
    Create AWS CDK TypeScript project with two stacks:

    **DataStack (infra/lib/data-stack.ts):**
    - S3 bucket for storing full profile JSON files (profile-results-{account}-{region})
    - Bucket lifecycle policy: transition to IA after 30 days, delete after 365 days
    - Export bucket ARN for cross-stack reference

    **ProfilerStack (infra/lib/profiler-stack.ts):**
    - VPC with 2 public subnets (for Fargate tasks that need to reach Supabase/AWS APIs)
    - ECS Fargate cluster (profiler-cluster)
    - Task definition placeholder (2 vCPU, 8GB RAM - actual container added in Plan 01-02)
    - Step Functions state machine skeleton with states: ValidateInput -> RunProfiler -> StoreResults -> Success (or HandleError -> Fail)
    - IAM roles: task execution role with ECR/CloudWatch access, task role with S3/Secrets Manager access
    - Security group allowing outbound HTTPS (443)

    **App entry (infra/bin/app.ts):**
    - Instantiate both stacks with proper dependencies (ProfilerStack depends on DataStack)

    Use CDK v2 constructs. Do NOT include actual ECS task definition container (that comes in 01-02 when Docker image exists).
  </action>
  <verify>
    cd infra && npm install && npx cdk synth
    # Should produce CloudFormation templates without errors
  </verify>
  <done>
    CDK synthesizes successfully. Stacks define VPC, ECS cluster, S3 bucket, Step Functions skeleton, and IAM roles.
  </done>
</task>

<task type="auto">
  <name>Task 2: Supabase Database Schema</name>
  <files>
    supabase/migrations/001_profile_schema.sql
  </files>
  <action>
    Create Supabase migration with tables from research schema:

    **Tables:**
    1. `data_sources` - Connection configurations (id, name, source_type, connection_config JSONB, created_at, updated_at, created_by)
    2. `datasets` - Tables within sources (id, source_id FK, database_name, table_name, schema_info JSONB, created_at, UNIQUE constraint)
    3. `profile_runs` - Job tracking (id, dataset_id FK, status enum, started_at, completed_at, error_message, step_functions_execution_arn, created_at)
    4. `profile_results` - Summary metrics (id, run_id FK, dataset_id FK, row_count, column_count, sampled, sample_size, s3_full_profile_uri, profiled_at)
    5. `column_profiles` - Per-column metrics (id, result_id FK, column_name, inferred_type, null_count, null_percentage, distinct_count, distinct_percentage, min_value, max_value, mean_value, median_value, std_dev, top_values JSONB, created_at)
    6. `profile_anomalies` - Detected anomalies (id, result_id FK, column_name, anomaly_type, severity, description, value, threshold, metadata JSONB, created_at)

    **Indexes:**
    - profile_runs(dataset_id), profile_runs(status)
    - profile_results(dataset_id)
    - column_profiles(result_id)
    - profile_anomalies(result_id), profile_anomalies(severity)

    **RLS:**
    - Enable RLS on all tables
    - Create permissive SELECT policy for authenticated users (all internal users can view)
    - Create INSERT/UPDATE/DELETE policies for authenticated users on data_sources

    Use gen_random_uuid() for IDs, TIMESTAMPTZ for timestamps.
  </action>
  <verify>
    # Validate SQL syntax (dry run without actual Supabase connection)
    cat supabase/migrations/001_profile_schema.sql | grep -E "CREATE TABLE|CREATE INDEX|ENABLE ROW LEVEL SECURITY"
    # Should show 6 tables, 7 indexes, 6 RLS enables
  </verify>
  <done>
    Migration file contains complete schema with all 6 tables, indexes, and RLS policies matching research specification.
  </done>
</task>

<task type="auto">
  <name>Task 3: Data Connector Classes</name>
  <files>
    agents/profiler/tools/connectors.py
    agents/profiler/schemas.py
    agents/profiler/requirements.txt
    agents/profiler/__init__.py
    agents/profiler/tools/__init__.py
  </files>
  <action>
    Create Python connector abstraction layer:

    **schemas.py:**
    - Pydantic models: DataSourceConfig (source_type: Literal['iceberg', 'redshift', 'athena'], connection_params: dict, database: str, table: str)
    - ConnectionTestResult (success: bool, message: str, row_count: Optional[int])

    **connectors.py:**
    - Abstract base class `DataConnector` with methods:
      - `get_sample(limit: int = 10000) -> pd.DataFrame`
      - `get_schema() -> dict[str, str]` (column_name -> type)
      - `get_row_count() -> int`
      - `test_connection() -> ConnectionTestResult`

    - `IcebergConnector(DataConnector)`:
      - Uses PyIceberg with Glue catalog
      - `load_catalog('default', type='glue', **{'client.region': region})`
      - Implements all methods per research patterns

    - `RedshiftConnector(DataConnector)`:
      - Uses boto3 redshift-data client
      - Async query execution with polling (max 5 min timeout)
      - Workgroup-based connection (serverless Redshift)

    - `AthenaConnector(DataConnector)`:
      - Uses boto3 athena client
      - Query execution with S3 output location
      - Reads CSV results from S3 into DataFrame

    - Factory function `get_connector(config: DataSourceConfig) -> DataConnector`

    **requirements.txt:**
    ```
    pyiceberg[glue,pyarrow,pandas]>=0.10.0
    boto3>=1.35.0
    pandas>=2.0.0
    pydantic>=2.0.0
    ```

    Include proper error handling: wrap AWS exceptions in ConnectorError with helpful messages.
  </action>
  <verify>
    cd agents/profiler && python -c "from tools.connectors import get_connector, IcebergConnector, RedshiftConnector, AthenaConnector; print('Imports OK')"
    # Should import without errors (AWS connections not tested, just syntax)
  </verify>
  <done>
    Connector module imports successfully. Factory function returns correct connector type. All three connector implementations follow research patterns.
  </done>
</task>

</tasks>

<verification>
1. `cd infra && npx cdk synth` produces valid CloudFormation
2. SQL migration file is syntactically valid
3. Python connector module imports without errors
4. All files exist at specified paths
</verification>

<success_criteria>
- CDK project synthesizes two stacks (DataStack, ProfilerStack)
- ProfilerStack contains: VPC, ECS cluster, Step Functions state machine, IAM roles
- DataStack contains: S3 bucket with lifecycle rules
- Supabase migration defines 6 tables with proper relationships and RLS
- Python connectors implement DataConnector interface for Iceberg, Redshift, Athena
- All code follows research patterns and recommendations
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-data-profiling/01-01-SUMMARY.md`
</output>
