---
phase: 01-foundation-data-profiling
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - agents/profiler/agent.py
  - agents/profiler/tools/profiler.py
  - agents/profiler/tools/anomaly.py
  - agents/profiler/Dockerfile
  - agents/profiler/entrypoint.py
  - infra/lib/profiler-stack.ts
  - backend/api/profiles/route.ts
  - backend/api/profiles/[id]/route.ts
  - backend/api/sources/route.ts
  - backend/api/sources/[id]/route.ts
  - backend/lib/supabase.ts
  - backend/package.json
autonomous: true

must_haves:
  truths:
    - "Profiler agent can be invoked with data source config and returns profile statistics"
    - "Profile results include completeness stats (nulls, distinct counts) and distribution metrics (min, max, mean, median, std dev)"
    - "Anomalies are detected and flagged with severity levels"
    - "API endpoints allow triggering profiles and retrieving results"
  artifacts:
    - path: "agents/profiler/agent.py"
      provides: "Strands agent definition with profiling tools"
      exports: ["profiler_agent"]
    - path: "agents/profiler/tools/profiler.py"
      provides: "ydata-profiling wrapper tool"
      exports: ["profile_table", "generate_profile"]
    - path: "agents/profiler/tools/anomaly.py"
      provides: "Statistical anomaly detection"
      exports: ["detect_anomalies", "detect_numeric_anomalies", "detect_categorical_anomalies"]
    - path: "agents/profiler/Dockerfile"
      provides: "Container image for Fargate"
      contains: "FROM python"
    - path: "backend/api/profiles/route.ts"
      provides: "Profile trigger and list endpoints"
      exports: ["POST", "GET"]
  key_links:
    - from: "agents/profiler/agent.py"
      to: "tools/profiler.py"
      via: "Strands @tool decorator"
      pattern: "tools=\\[profile_table"
    - from: "agents/profiler/tools/profiler.py"
      to: "tools/connectors.py"
      via: "get_connector call"
      pattern: "from .connectors import get_connector"
    - from: "backend/api/profiles/route.ts"
      to: "AWS Step Functions"
      via: "SDK startExecution"
      pattern: "sfnClient\\.send.*StartExecution"
---

<objective>
Build the Data Profiler Agent that scans data sources and generates statistical profiles.

Purpose: Implement the core profiling capability using Strands Agents SDK with ydata-profiling, packaged as a Fargate container, with API endpoints for triggering and retrieving profiles.

Output: Functional profiler agent container, updated CDK with task definition, and working API endpoints.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation-data-profiling/01-RESEARCH.md
@.planning/phases/01-foundation-data-profiling/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Profiler Agent with Tools</name>
  <files>
    agents/profiler/agent.py
    agents/profiler/tools/profiler.py
    agents/profiler/tools/anomaly.py
    agents/profiler/tools/__init__.py
    agents/profiler/requirements.txt
  </files>
  <action>
    Implement the Data Profiler Agent using Strands Agents SDK:

    **tools/profiler.py:**
    - `@tool` decorated function `profile_table(source_type, connection_params_json, database, table, sample_size=10000) -> str`:
      - Parse connection_params from JSON string
      - Use get_connector() from connectors.py to get appropriate connector
      - Call get_sample() to retrieve data
      - Use ydata-profiling ProfileReport with `minimal=True` (critical for performance)
      - Return JSON string with profile statistics
    - Helper function `generate_profile(df, table_name, sample_threshold=100000) -> dict`:
      - Sample if >100K rows
      - Configure ProfileReport: minimal=True, explorative=False, correlations={'pearson': {'calculate': True}}
      - Return dict with 'summary' (for PostgreSQL) and 'full_profile' (for S3)

    **tools/anomaly.py:**
    - `detect_numeric_anomalies(series, column_name) -> List[dict]`:
      - Z-score outlier detection (>3 std dev, flag if >5% outliers)
      - IQR outlier detection (flag if >10% outliers)
      - High null rate detection (flag if >20%)
      - Return list of anomaly dicts with: column, type, method, value, threshold, severity, description
    - `detect_categorical_anomalies(series, column_name) -> List[dict]`:
      - High cardinality detection (>90% unique and >100 values)
      - Single value dominance (>95% same value)
    - `@tool` decorated function `detect_anomalies(profile_json: str) -> str`:
      - Parse profile JSON
      - Run numeric/categorical anomaly detection on each column
      - Return JSON string with all detected anomalies

    **agent.py:**
    - Import BedrockModel from strands.models.bedrock
    - Configure model: anthropic.claude-sonnet-4-20250514, us-east-1
    - Define profiler_agent with system prompt explaining its role (profile tables, detect anomalies, summarize findings)
    - Register tools: [profile_table, detect_anomalies]

    Update requirements.txt to add:
    ```
    ydata-profiling>=4.18.0
    strands-agents>=1.0.0
    scipy>=1.0.0
    numpy>=2.0.0
    ```

    Follow research patterns exactly. Use scipy.stats.zscore for z-score calculation.
  </action>
  <verify>
    cd agents/profiler && python -c "
from agent import profiler_agent
from tools.profiler import profile_table
from tools.anomaly import detect_anomalies
print('Agent and tools imported successfully')
print(f'Tools registered: {len(profiler_agent.tools)}')"
  </verify>
  <done>
    Profiler agent imports successfully with 2 tools registered. Tools implement ydata-profiling wrapper and statistical anomaly detection.
  </done>
</task>

<task type="auto">
  <name>Task 2: Fargate Container and CDK Update</name>
  <files>
    agents/profiler/Dockerfile
    agents/profiler/entrypoint.py
    infra/lib/profiler-stack.ts
  </files>
  <action>
    Create Fargate container and update CDK to deploy it:

    **Dockerfile:**
    - Base image: python:3.11-slim
    - Install system deps: gcc (for scipy compilation)
    - Copy requirements.txt and install with pip
    - Copy agent code
    - Set PYTHONPATH to include agents directory
    - Entrypoint: python entrypoint.py

    **entrypoint.py:**
    - Read environment variables: SOURCE_TYPE, DATABASE, TABLE, CONNECTION_PARAMS (JSON), SUPABASE_URL, SUPABASE_KEY, S3_BUCKET, RUN_ID
    - Initialize Supabase client
    - Update profile_runs status to 'running'
    - Call profiler_agent with constructed prompt to profile the specified table
    - Parse agent response, extract profile data
    - Store summary in profile_results and column_profiles tables
    - Upload full profile JSON to S3
    - Update profile_runs status to 'completed' (or 'failed' on error)
    - Proper error handling with status updates on failure

    **Update infra/lib/profiler-stack.ts:**
    - Add ECR repository for profiler image
    - Update task definition with container:
      - Image from ECR
      - Environment variables from Secrets Manager (SUPABASE_URL, SUPABASE_KEY)
      - Environment variable for S3_BUCKET from DataStack
      - Logging to CloudWatch
    - Update Step Functions state machine:
      - ValidateInput: Check required fields exist
      - RunProfiler: ecs:runTask.sync with container overrides for SOURCE_TYPE, DATABASE, TABLE, CONNECTION_PARAMS, RUN_ID
      - StoreResults: Lambda to update final status (or inline in Fargate task)
      - Error handling with SNS notification
  </action>
  <verify>
    # Check Dockerfile syntax
    docker build -t profiler-test agents/profiler --dry-run 2>&1 | head -5 || echo "Docker not available, checking file exists"
    ls -la agents/profiler/Dockerfile agents/profiler/entrypoint.py
    # Verify CDK still synthesizes
    cd infra && npx cdk synth --quiet
  </verify>
  <done>
    Dockerfile builds (or exists if Docker unavailable). Entrypoint handles full workflow. CDK synthesizes with ECR repository and updated task definition.
  </done>
</task>

<task type="auto">
  <name>Task 3: Backend API Endpoints</name>
  <files>
    backend/api/profiles/route.ts
    backend/api/profiles/[id]/route.ts
    backend/api/sources/route.ts
    backend/api/sources/[id]/route.ts
    backend/lib/supabase.ts
    backend/package.json
    backend/tsconfig.json
  </files>
  <action>
    Create Node.js/TypeScript API for profile management (standalone backend, not Next.js yet):

    **backend/lib/supabase.ts:**
    - Initialize Supabase client with service role key (for RLS bypass in backend)
    - Export typed client using generated types

    **backend/api/sources/route.ts:**
    - GET: List all data_sources with dataset counts
    - POST: Create new data_source, validate source_type is one of ['iceberg', 'redshift', 'athena']
    - Return proper error responses (400 for validation, 500 for server errors)

    **backend/api/sources/[id]/route.ts:**
    - GET: Get single data_source by ID with its datasets
    - PUT: Update data_source (name, connection_config)
    - DELETE: Delete data_source (cascades to datasets, runs, results)

    **backend/api/profiles/route.ts:**
    - GET: List profile_runs with filters (dataset_id, status), include latest result summary
    - POST: Trigger new profile run:
      1. Validate dataset_id exists
      2. Create profile_runs record with status='pending'
      3. Start Step Functions execution via AWS SDK (@aws-sdk/client-sfn)
      4. Update profile_runs with execution ARN
      5. Return run_id for polling

    **backend/api/profiles/[id]/route.ts:**
    - GET: Get profile_run by ID with full results:
      - Include profile_results
      - Include column_profiles array
      - Include profile_anomalies array
      - If status='completed', return full data
      - If status='running', return status only (for polling)

    **package.json:**
    - Dependencies: @supabase/supabase-js, @aws-sdk/client-sfn, zod (validation)
    - Scripts for local development

    Use Zod for request validation. Return consistent JSON structure: { data, error }.
  </action>
  <verify>
    cd backend && npm install && npx tsc --noEmit
    # TypeScript compiles without errors
  </verify>
  <done>
    Backend API compiles successfully. Endpoints handle CRUD for sources and profiles. Profile trigger integrates with Step Functions.
  </done>
</task>

</tasks>

<verification>
1. `cd agents/profiler && python -c "from agent import profiler_agent"` succeeds
2. Dockerfile exists and has correct structure
3. CDK synthesizes with ECR repository and task definition
4. `cd backend && npx tsc --noEmit` succeeds
5. API routes export correct HTTP methods
</verification>

<success_criteria>
- Profiler agent uses Strands SDK with profile_table and detect_anomalies tools
- ydata-profiling runs in minimal mode with sampling for large datasets
- Anomaly detection implements z-score, IQR, null rate, cardinality checks
- Fargate container handles full profiling workflow with Supabase/S3 storage
- API endpoints allow: list/create sources, trigger profiles, poll status, retrieve results
- Step Functions orchestrates Fargate task execution with error handling
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-data-profiling/01-02-SUMMARY.md`
</output>
