---
phase: 03-column-level-lineage
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - agents/lineage/tools/aws_extractor.py
  - agents/lineage/tools/lineage_store.py
  - lambdas/lineage_extractor/handler.py
  - lambdas/lineage_extractor/requirements.txt
  - lambdas/openlineage_consumer/handler.py
  - lambdas/openlineage_consumer/requirements.txt
  - infra/lib/lineage-stack.ts
autonomous: true

must_haves:
  truths:
    - "System extracts query history from Redshift SVL_STATEMENTTEXT"
    - "System extracts query history from Athena via boto3 API"
    - "Extracted queries are parsed for column lineage and stored"
    - "Lineage extraction runs on a schedule via Lambda"
    - "Duplicate queries are detected and skipped via sql_hash"
    - "System accepts OpenLineage events from external tools (Airflow, Spark, dbt) and stores column lineage"
  artifacts:
    - path: "agents/lineage/tools/aws_extractor.py"
      provides: "Query extraction from Redshift and Athena"
      exports: ["extract_redshift_queries", "extract_athena_queries"]
    - path: "agents/lineage/tools/lineage_store.py"
      provides: "Store parsed lineage to Supabase"
      exports: ["store_lineage_result", "upsert_lineage_node", "create_lineage_edge"]
    - path: "lambdas/lineage_extractor/handler.py"
      provides: "Scheduled Lambda for batch lineage extraction"
      exports: ["handler"]
    - path: "lambdas/openlineage_consumer/handler.py"
      provides: "OpenLineage event consumer endpoint"
      exports: ["handler"]
    - path: "infra/lib/lineage-stack.ts"
      provides: "CDK stack for lineage infrastructure"
      exports: ["LineageStack"]
  key_links:
    - from: "agents/lineage/tools/aws_extractor.py"
      to: "Redshift Data API"
      via: "boto3 redshift-data client"
      pattern: "redshift_data\\.execute_statement"
    - from: "agents/lineage/tools/aws_extractor.py"
      to: "Athena API"
      via: "boto3 athena client"
      pattern: "athena\\.list_query_executions|athena\\.get_query_execution"
    - from: "lambdas/lineage_extractor/handler.py"
      to: "agents/lineage/tools/*"
      via: "batch extraction orchestration"
      pattern: "extract.*queries.*store_lineage"
    - from: "lambdas/openlineage_consumer/handler.py"
      to: "agents/lineage/tools/lineage_store.py"
      via: "store_lineage_result function"
      pattern: "store_lineage_result"
---

<objective>
Implement AWS lineage extraction from Redshift and Athena query logs, OpenLineage event consumption for external tool integration, with batch processing and storage to the lineage graph.

Purpose: Automatically populate the lineage graph by extracting query history from AWS data sources, parsing SQL for column dependencies, and accepting OpenLineage events from external tools (Airflow, Spark, dbt). This enables users to see data flow without manual lineage definition and integrates with the broader data ecosystem.

Output: AWS extraction tools, OpenLineage consumer endpoint, lineage storage functions, and scheduled Lambda for batch extraction.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-column-level-lineage/03-RESEARCH.md
@.planning/phases/03-column-level-lineage/03-01-SUMMARY.md
@agents/profiler/tools/connectors.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: AWS Extractor and Lineage Store Tools</name>
  <files>
    agents/lineage/tools/aws_extractor.py
    agents/lineage/tools/lineage_store.py
  </files>
  <action>
    Implement AWS query extraction and lineage storage tools:

    **tools/aws_extractor.py:**

    `extract_redshift_queries(workgroup: str, region: str = "us-east-1", since_hours: int = 24) -> str`:
    - Use boto3 redshift-data client (serverless-friendly, matches profiler pattern)
    - Query SVL_STATEMENTTEXT for recent DDL/DML:
      ```sql
      SELECT
          userid, xid, starttime,
          LISTAGG(text) WITHIN GROUP (ORDER BY sequence) as query_text
      FROM svl_statementtext
      WHERE starttime > DATEADD(hour, -{since_hours}, GETDATE())
      AND type IN ('DDL', 'QUERY')
      GROUP BY userid, xid, starttime
      ORDER BY starttime DESC
      LIMIT 1000
      ```
    - Filter out utility queries (SHOW, DESCRIBE, SET, etc.)
    - Return JSON with list of {id, query_text, executed_at, database}

    `extract_athena_queries(region: str = "us-east-1", since_hours: int = 24, max_results: int = 50) -> str`:
    - Use boto3 athena client list_query_executions + get_query_execution
    - Filter for SUCCEEDED queries within time window
    - Extract query text, database, timestamp
    - Skip queries that are just catalog operations
    - Return JSON with list of {id, query_text, database, executed_at}

    `get_glue_catalog_schema(database: str, table: str, region: str = "us-east-1") -> str`:
    - Use boto3 glue client get_table
    - Extract column names and types from StorageDescriptor
    - Return JSON schema in SQLGlot MappingSchema format
    - This provides schema context for accurate lineage extraction

    **tools/lineage_store.py:**

    `upsert_lineage_node(node_type: str, namespace: str, name: str, parent_id: str = None, data_type: str = None, metadata: str = "{}") -> str`:
    - Insert or update node in lineage_nodes table
    - Use ON CONFLICT (node_type, namespace, name, parent_id) DO UPDATE
    - Return node ID (new or existing)

    `create_lineage_edge(source_id: str, target_id: str, edge_type: str, transformation_type: str, transformation_subtype: str, description: str = None, job_id: str = None, sql_hash: str = None) -> str`:
    - Insert edge in lineage_edges table
    - Skip if duplicate (source_id, target_id, job_id combination exists)
    - Return edge ID or existing edge ID if duplicate

    `store_lineage_result(lineage_result: str, job_id: str = None) -> str`:
    - Parse SQLLineageResult JSON
    - Upsert source table nodes and column nodes
    - Upsert target table node and column nodes
    - Create edges between source and target columns with transformation info
    - Return JSON with counts: {nodes_created, edges_created, duplicates_skipped}

    `store_openlineage_event(event: dict) -> str`:
    - Parse OpenLineage RunEvent structure
    - Extract ColumnLineageDatasetFacet from outputs[].facets.columnLineage
    - For each output dataset with column lineage:
      - Create dataset node for output
      - For each field in columnLineage.fields:
        - Create column node for output field
        - For each inputField in field.inputFields:
          - Create/lookup source dataset and column nodes
          - Create edge with transformation type from transformations[]
    - Return JSON with counts: {datasets_created, columns_created, edges_created}

    `check_sql_processed(sql_hash: str) -> str`:
    - Check if SQL hash already exists in lineage_edges
    - Return JSON with {processed: bool, edge_count: int}
    - Used to skip duplicate processing

    Update agents/lineage/tools/__init__.py to export new functions.
  </action>
  <verify>
    cd agents/lineage && python -c "
from tools.aws_extractor import extract_redshift_queries, extract_athena_queries, get_glue_catalog_schema
from tools.lineage_store import upsert_lineage_node, create_lineage_edge, store_lineage_result, store_openlineage_event
print('AWS extractor and lineage store tools imported successfully')"
  </verify>
  <done>
    AWS extractor tools query Redshift SVL_STATEMENTTEXT and Athena query history. Lineage store tools persist parsed lineage to Supabase with deduplication via sql_hash. store_openlineage_event parses OpenLineage events and extracts column lineage.
  </done>
</task>

<task type="auto">
  <name>Task 2: Lineage Extractor Lambda, OpenLineage Consumer, and CDK Stack</name>
  <files>
    lambdas/lineage_extractor/handler.py
    lambdas/lineage_extractor/requirements.txt
    lambdas/openlineage_consumer/handler.py
    lambdas/openlineage_consumer/requirements.txt
    infra/lib/lineage-stack.ts
  </files>
  <action>
    Create Lambdas for scheduled lineage extraction and OpenLineage consumption, plus supporting infrastructure:

    **lambdas/lineage_extractor/handler.py:**

    Main handler for batch lineage extraction:
    - Parse event for extraction config:
      - source_type: "redshift" | "athena" | "all"
      - since_hours: int (default 24)
      - workgroup: str (for Redshift, from env if not provided)
      - region: str (default "us-east-1")
    - Create lineage_run record with status "running"
    - For each source:
      1. Extract queries using aws_extractor
      2. Get schema context from Glue Catalog
      3. For each query:
         - Check if sql_hash already processed
         - Parse SQL using sql_parser.parse_sql_lineage
         - Store results using lineage_store.store_lineage_result
      4. Track counts: queries_processed, edges_created, errors
    - Update lineage_run with status "completed" or "failed"
    - Return summary JSON

    Error handling:
    - Continue on individual query parse failures (log and skip)
    - Fail run only on infrastructure errors (Supabase down, etc.)
    - Store error_message in lineage_run for debugging

    Use Lambda Powertools for logging and tracing:
    ```python
    from aws_lambda_powertools import Logger, Tracer
    from aws_lambda_powertools.utilities.typing import LambdaContext

    logger = Logger()
    tracer = Tracer()

    @logger.inject_lambda_context
    @tracer.capture_lambda_handler
    def handler(event: dict, context: LambdaContext) -> dict:
        ...
    ```

    **lambdas/lineage_extractor/requirements.txt:**
    ```
    boto3>=1.35.0
    aws-lambda-powertools>=3.0.0
    sqlglot>=26.0.0
    pydantic>=2.0.0
    httpx>=0.28.0
    ```

    **lambdas/openlineage_consumer/handler.py:**

    OpenLineage event consumer endpoint (INT-02 requirement):
    - Accept POST requests with OpenLineage RunEvent JSON body
    - Validate event structure:
      - Required: eventType, eventTime, run, job, producer
      - For lineage: outputs[].facets.columnLineage must exist
    - Parse ColumnLineageDatasetFacet from facets:
      - Schema: https://openlineage.io/spec/facets/1-2-0/ColumnLineageDatasetFacet.json
      - Extract fields[].inputFields[] for column mappings
      - Extract transformations[] for transformation type/description
    - Call store_openlineage_event from lineage_store to persist
    - Return 200 with {run_id, lineage_stored: bool, stats: {datasets, columns, edges}}
    - Return 400 for invalid event structure
    - Return 422 for valid event but no column lineage facet (still accept, just log)

    Example OpenLineage RunEvent with column lineage:
    ```json
    {
      "eventType": "COMPLETE",
      "eventTime": "2024-01-15T10:00:00Z",
      "run": {"runId": "uuid"},
      "job": {"namespace": "airflow", "name": "etl_pipeline"},
      "producer": "https://airflow.example.com",
      "outputs": [{
        "namespace": "redshift://analytics",
        "name": "fact_orders",
        "facets": {
          "columnLineage": {
            "_producer": "...",
            "_schemaURL": "...",
            "fields": {
              "total_amount": {
                "inputFields": [
                  {"namespace": "redshift://raw", "name": "orders", "field": "amount"},
                  {"namespace": "redshift://raw", "name": "orders", "field": "tax"}
                ],
                "transformationDescription": "amount + tax",
                "transformationType": "DIRECT"
              }
            }
          }
        }
      }]
    }
    ```

    **lambdas/openlineage_consumer/requirements.txt:**
    ```
    boto3>=1.35.0
    aws-lambda-powertools>=3.0.0
    pydantic>=2.0.0
    httpx>=0.28.0
    ```

    **infra/lib/lineage-stack.ts:**

    CDK stack for lineage infrastructure:

    1. Lambda function for lineage_extractor:
       - Runtime: Python 3.11
       - Timeout: 15 minutes (long extraction jobs)
       - Memory: 1024 MB (SQLGlot parsing needs memory)
       - Environment:
         - SUPABASE_URL, SUPABASE_KEY from Secrets Manager (use existing pattern from dq-recommender-stack)
         - REDSHIFT_WORKGROUP from environment variable
       - IAM permissions:
         - redshift-data:ExecuteStatement, redshift-data:GetStatementResult, redshift-data:DescribeStatement
         - athena:ListQueryExecutions, athena:GetQueryExecution
         - glue:GetTable, glue:GetDatabase
         - secretsmanager:GetSecretValue (for Supabase credentials)

    2. Lambda function for openlineage_consumer:
       - Runtime: Python 3.11
       - Timeout: 30 seconds (single event processing)
       - Memory: 256 MB
       - Environment:
         - SUPABASE_URL, SUPABASE_KEY from Secrets Manager
       - IAM permissions:
         - secretsmanager:GetSecretValue (for Supabase credentials)

    3. EventBridge Rule for scheduled extraction:
       - Schedule: rate(1 hour) - hourly extraction
       - Target: lineage_extractor Lambda
       - Input: { "source_type": "all", "since_hours": 2 } (overlap for safety)
       - Enabled by default

    4. API Gateway HTTP endpoint for lineage operations:
       - POST /lineage/extract -> lineage_extractor Lambda (manual trigger)
         - Request body: { source_type, since_hours, workgroup }
       - POST /api/openlineage -> openlineage_consumer Lambda (INT-02)
         - Request body: OpenLineage RunEvent JSON
         - CORS enabled for external callers
       - CORS enabled for frontend

    Export Lambda ARNs, API endpoint URL, OpenLineage endpoint URL, and EventBridge rule ARN.
  </action>
  <verify>
    # Check Lambda handlers syntax
    python -c "import ast; ast.parse(open('lambdas/lineage_extractor/handler.py').read())"
    python -c "import ast; ast.parse(open('lambdas/openlineage_consumer/handler.py').read())"

    # CDK synth
    cd infra && npx cdk synth --quiet
  </verify>
  <done>
    Lineage extractor Lambda processes query logs from Redshift/Athena on schedule. OpenLineage consumer Lambda accepts RunEvents at POST /api/openlineage and extracts ColumnLineageDatasetFacet (INT-02). EventBridge runs hourly extraction. CDK stack includes all IAM permissions for AWS service access.
  </done>
</task>

</tasks>

<verification>
1. AWS extractor tools import without errors
2. Lineage store tools use Supabase client correctly
3. store_openlineage_event parses ColumnLineageDatasetFacet correctly
4. Lambda handlers have proper error handling and logging
5. CDK synth produces both Lambdas, EventBridge rule, and API Gateway with /api/openlineage route
6. IAM permissions cover Redshift Data API, Athena, Glue Catalog, and Secrets Manager
7. OpenLineage consumer validates event structure and returns appropriate status codes
</verification>

<success_criteria>
- Redshift extraction queries SVL_STATEMENTTEXT with proper filtering
- Athena extraction uses list_query_executions + get_query_execution pattern
- Glue Catalog schema lookup provides context for SQLGlot parsing
- Lineage storage deduplicates via sql_hash to avoid reprocessing
- Lambda runs hourly via EventBridge with 2-hour lookback window
- Manual extraction available via API endpoint
- POST /api/openlineage accepts OpenLineage RunEvents and stores column lineage (INT-02)
- External tools (Airflow, Spark, dbt) can emit lineage to our system
- Errors logged but don't stop batch processing
</success_criteria>

<output>
After completion, create `.planning/phases/03-column-level-lineage/03-02-SUMMARY.md`
</output>
